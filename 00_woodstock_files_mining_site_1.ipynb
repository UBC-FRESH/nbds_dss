{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2952d534-038b-4367-9b3f-7603076adc1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook imports raw ws3 input data, reformats and monkey-patches the data, and exports Woodstock formatted input data files (which we will use in other DSS notebooks for this case as the input data files). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8c11e-dbc1-48f1-996c-4109720d5efe",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f49e412-0429-4b54-aa90-b3b1ce485693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23141f45-1925-43df-8bd4-0dfe8c3fc8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import ws3.forest, ws3.core\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial, wraps\n",
    "import distance\n",
    "import operator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd44bc0-6b66-4dd1-9137-8ab51ec3b6b0",
   "metadata": {},
   "source": [
    "Define some key model parameters (will get used but defined here up top for convenience)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e46e712-2b73-41ee-bdb7-767683a16d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "period_length = 10\n",
    "max_age =  1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22db0e20-6b4b-4fed-8538-ee752a7f20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths\n",
    "# TSA 03, 12, 14, 20, 24\n",
    "vdyp_curves_smooth_tsa03_path = './data/vdyp_curves_smooth-tsa03.feather'\n",
    "vdyp_curves_smooth_tsa12_path = './data/vdyp_curves_smooth-tsa12.feather'\n",
    "vdyp_curves_smooth_tsa14_path = './data/vdyp_curves_smooth-tsa14.feather'\n",
    "vdyp_curves_smooth_tsa20_path = './data/vdyp_curves_smooth-tsa20.feather'\n",
    "vdyp_curves_smooth_tsa24_path = './data/vdyp_curves_smooth-tsa24.feather'\n",
    "\n",
    "stands_shape_file_path = './data/mining_site_1_large_data/mining_site_1.shp'\n",
    "\n",
    "\n",
    "# Output paths\n",
    "woodstock_model_files_lan_path = './data/woodstock_model_files_mining_site_1/mining_site_1.lan'\n",
    "woodstock_model_files_are_path = './data/woodstock_model_files_mining_site_1/mining_site_1.are'\n",
    "woodstock_model_files_yld_path = './data/woodstock_model_files_mining_site_1/mining_site_1.yld'\n",
    "woodstock_model_files_act_path = './data/woodstock_model_files_mining_site_1/mining_site_1.act'\n",
    "woodstock_model_files_trn_path = './data/woodstock_model_files_mining_site_1/mining_site_1.trn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8aff4de-b9e5-4e42-86ea-a2de0829b422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yld_vdyp_03 = pd.read_feather(vdyp_curves_smooth_tsa03_path)\n",
    "yld_vdyp_12 = pd.read_feather(vdyp_curves_smooth_tsa12_path)\n",
    "yld_vdyp_14 = pd.read_feather(vdyp_curves_smooth_tsa14_path)\n",
    "yld_vdyp_20 = pd.read_feather(vdyp_curves_smooth_tsa20_path)\n",
    "yld_vdyp_24 = pd.read_feather(vdyp_curves_smooth_tsa24_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22316a-e1b1-4aff-8cb7-acdb520e985d",
   "metadata": {},
   "source": [
    "# Import and reformat inventory and yield input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167c8e8-ac87-4e9d-9a18-06d967d7949a",
   "metadata": {},
   "source": [
    "Read forest inventory data into memory (vector polygon GIS data layer with attribute table, in ESRI Shapefile format). This dataset represents timber supply area (TSA) 04 in British Columbia. We monkey-patch the inventory data here to make it line up nicely with what we need downstream as input for the ws3 model (i.e., changes we make here to the in-memory dataset are not saved to the original dataset on disk). Most of what we are doing here is setting up the _theme_ columns in the attribute table, which should help newer ws3 users make the connection between input data and the landscape themes in ws3 model further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f617e9-1d23-4e80-9bf1-b77f2dcab25e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 1.7 minutes to run this script.\n"
     ]
    }
   ],
   "source": [
    "Start = time.time()\n",
    "stands = gpd.read_file(stands_shape_file_path)\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# stands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20712755-95da-481a-aef1-ffc28044d636",
   "metadata": {},
   "source": [
    "Import CANFI tree species lookup table (associates tree species names with integer numerical values, which we use as theme data values in the ws3 model), and insert species code values into the yield curve dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9b166-76df-4105-a910-848238bc9935",
   "metadata": {},
   "source": [
    "Here we are removing the satnds with wrong value in the age column (equals with 42 stands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150470fc-97bc-4fde-af92-79224b01fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands = stands.loc[stands['Age_2024'] != 2024]\n",
    "# stands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1214a715-f916-466a-a930-0539815deeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "canfi_map = {'AC':1211, \n",
    "             'AT':1201,\n",
    "             'ACT':1201,\n",
    "             'BL':304,\n",
    "             'B' :304, #based on 00_data_prep_Resultant.ipynp\n",
    "             'BA': 304, # added manually\n",
    "             'EP':1303, \n",
    "             'FDI':500, \n",
    "             'H' :400,\n",
    "             'HW':402,\n",
    "             'HM':403,\n",
    "             'PL':204, \n",
    "             'PLI':204,\n",
    "             'PA':204,\n",
    "             'SB':101, \n",
    "             'SE':104, \n",
    "             'SW':105, \n",
    "             'SX':100,\n",
    "             'S':100,\n",
    "             'SXS':100, # added manually\n",
    "             'SXW':100, # added manually\n",
    "             'CW': 701, # Eastern white-cedar           \n",
    "             'AT+SX':1201,\n",
    "             'SX+AT':100,\n",
    "             'SX+ACT' :100,\n",
    "             'AT+PLI' :1201,\n",
    "             'PLI+AT' : 204,\n",
    "             'FDI' : 500 # Douglas fir             \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b0e9bf-35a2-41ae-b157-cb7ae96d0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aspen = ['AC', 'ACT', 'AT', 'EP', 'VB', 'MB', 'AT+SX']\n",
    "Bal = ['B', 'BA', 'BG', 'BL']\n",
    "Cedar = ['CW', 'YC']\n",
    "Alder = ['D', 'DR']\n",
    "DougFir = ['F', 'FD', 'FDC', 'FDI']\n",
    "Hem = ['H', 'HM', 'HW']\n",
    "Pine = ['PA', 'PL', 'PLC', 'PW', 'PLI', 'PY']\n",
    "Spruce = ['S', 'SS', 'SW', 'SX', 'SE', 'SXW', 'SB', 'SXS', 'SX+AT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d4a58-1058-473e-ab7b-697e026e2efa",
   "metadata": {},
   "source": [
    "Burn CANFI species codes into yield data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f9db61-6c8c-4e41-baa8-695451f3f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canfi_species(stratum_code):\n",
    "    s = stratum_code.split('_')[-1].split('+')[0]\n",
    "    result = canfi_map[s]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8be4684-7d40-4616-9df8-f334cf15e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_stand(r, lexmatch=False, lexmatch_fieldname_suffix='_lexmatch'):\n",
    "    result = ''\n",
    "    if lexmatch:\n",
    "        result += 3 * r['BEC_ZONE_CODE%s' % lexmatch_fieldname_suffix]\n",
    "        result += '_'\n",
    "        result += 2 * r['SPECIES_CD_1%s' % lexmatch_fieldname_suffix]\n",
    "        if r.BCLCS_LEVE == 'TM' and r.SPECIES__1 != None:\n",
    "            result += '+' + r['SPECIES_CD_2%s' % lexmatch_fieldname_suffix]\n",
    "    else:\n",
    "        result += r.BEC_ZONE_C\n",
    "        result += '_'\n",
    "        result += r.SPECIES_CD\n",
    "        if r.BCLCS_LEVE == 'TM' and r.SPECIES__1 != None:\n",
    "            result += '+' + r.SPECIES__1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a53289f-2fcd-4097-98a2-45a3c84b066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       TSA_NUMBER  FEATURE_AR BCLCS_LEVE BEC_ZONE_C  SITE_INDEX SPECIES_CD  \\\n",
      "486            20         0.0         SL        SBS         0.0       None   \n",
      "535            20         0.0         SL        SBS         0.0       None   \n",
      "1136           20         0.0         SL        SBS         0.0       None   \n",
      "1137           20         0.0         SL        SBS         0.0       None   \n",
      "1195           20         0.0         SL        SBS         0.0       None   \n",
      "...           ...         ...        ...        ...         ...        ...   \n",
      "419090         14         0.0         SL       ESSF         0.0       None   \n",
      "419131         14         0.0         SL        SBS         0.0       None   \n",
      "419205         14         0.0       None        SBS         0.0       None   \n",
      "420465         03         0.0         HE       ESSF         0.0       None   \n",
      "420469         12         0.0         HE        ICH         0.0       None   \n",
      "\n",
      "        SPECIES_PC SPECIES__1  SPECIES__2   Shape_Leng     Shape_Area  \\\n",
      "486            0.0       None         0.0   359.760710    5659.966223   \n",
      "535            0.0       None         0.0  1932.646721   71081.944394   \n",
      "1136           0.0       None         0.0   907.510631   14944.648502   \n",
      "1137           0.0       None         0.0   355.407962    2026.599303   \n",
      "1195           0.0       None         0.0  4027.349549  494579.052512   \n",
      "...            ...        ...         ...          ...            ...   \n",
      "419090         0.0       None         0.0   400.085499    1565.711107   \n",
      "419131         0.0       None         0.0   371.260468    1483.341541   \n",
      "419205         0.0       None         0.0   517.057639    3914.915233   \n",
      "420465         0.0       None         0.0   290.594187    1238.363406   \n",
      "420469         0.0       None         0.0   393.706470    2418.313992   \n",
      "\n",
      "       contclass      rollup              netdown  THLB_Area  Block_ID  \\\n",
      "486            C        THLB                 THLB   0.565997       554   \n",
      "535            C        THLB                 THLB   7.108194       608   \n",
      "1136           C        THLB                 THLB   1.494465      1227   \n",
      "1137           C        THLB                 THLB   0.202660      1228   \n",
      "1195           C        THLB                 THLB  49.457905      1288   \n",
      "...          ...         ...                  ...        ...       ...   \n",
      "419090         N  6_Riparian  6_03_Wetland_Buffer   0.000000    549559   \n",
      "419131         N  6_Riparian  6_03_Wetland_Buffer   0.000000    549600   \n",
      "419205         N  6_Riparian  6_03_Wetland_Buffer   0.000000    549674   \n",
      "420465         N  6_Riparian  6_03_Wetland_Buffer   0.000000    550938   \n",
      "420469         N  6_Riparian  6_03_Wetland_Buffer   0.000000    550942   \n",
      "\n",
      "        Age_2024                                           geometry  \n",
      "486            5  POLYGON ((957560.221 1030219.756, 957559.220 1...  \n",
      "535            5  POLYGON ((957692.000 1030566.000, 957739.260 1...  \n",
      "1136           6  POLYGON ((962367.240 1025445.250, 962362.360 1...  \n",
      "1137           6  POLYGON ((961229.488 1027079.247, 961092.269 1...  \n",
      "1195           6  POLYGON ((964985.180 1026598.360, 964995.060 1...  \n",
      "...          ...                                                ...  \n",
      "419090         7  POLYGON ((984393.073 1026997.156, 984400.520 1...  \n",
      "419131         7  POLYGON ((994283.375 1024198.434, 994290.278 1...  \n",
      "419205         2  POLYGON ((986720.202 1047322.853, 986718.069 1...  \n",
      "420465        32  POLYGON ((923993.610 1124979.470, 924002.550 1...  \n",
      "420469         2  POLYGON ((900748.119 1139725.492, 900731.951 1...  \n",
      "\n",
      "[3973 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check whether'SPECIES_CD' is None\n",
    "\n",
    "print(stands[stands['SPECIES_CD'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025771aa-be82-40fc-9b16-266ff18d7d95",
   "metadata": {},
   "source": [
    "There are no species data for 3,973 stands, so we used the dominant species for each BEC zone in each TSA to fill in the missing information.\n",
    "\n",
    "| TSA_NUMBER | BEC_ZONE_C    | SPECIES_CD   |\n",
    "|------------|---------------|--------------|\n",
    "| 3          | BAFA          | BL           |\n",
    "| 3          | CWH           | BL           |\n",
    "| 3          | ESSF          | BL           |\n",
    "| 3          | ICH           | SX           |\n",
    "| 3          | SBS           | AT           |\n",
    "| 12         | BAFA          | BL           |\n",
    "| 12         | CWH           | HW           |\n",
    "| 12         | ESSF          | B            |\n",
    "| 12         | ICH           | AT           |\n",
    "| 12         | MH            | B            |\n",
    "| 14         | ESSF          | BL           |\n",
    "| 14         | SBS           | SX           |\n",
    "| 20         | BAFA          | BL           |\n",
    "| 20         | CWH           | HM           |\n",
    "| 20         | ESSF          | BL           |\n",
    "| 20         | SBS           | SX           |\n",
    "| 24         | ESSF          | SE           |\n",
    "| 24         | SBS         AT           |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce731f9e-1740-45ef-a330-389b558ed807",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_mapping = {\n",
    "('03',\t'BAFA'): \t'BL',\n",
    "('03',\t'CWH'):\t'BL',\n",
    "('03',\t'ESSF'):\t'BL',\n",
    "('03',\t'ICH'):\t'SX',\n",
    "('03',\t'SBS'):\t'AT',\n",
    "('12',\t'BAFA'):\t'BL',\n",
    "('12',\t'CWH'):\t'HW',\n",
    "('12',\t'ESSF'):\t'B',\n",
    "('12',\t'ICH'):\t'AT',\n",
    "('12',\t'MH'):\t'B',\n",
    "('14',\t'ESSF'):\t'BL',\n",
    "('14',\t'SBS'):\t'SX',\n",
    "('20',\t'BAFA'):\t'BL',\n",
    "('20',\t'CWH'):\t'HM',\n",
    "('20',\t'ESSF'):\t'BL',\n",
    "('20',\t'SBS'):\t'SX',\n",
    "('24',\t'ESSF'):\t'SE',\n",
    "('24',\t'SBS'):\t'AT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d0bd06-0fab-4658-9af0-a0c36a90e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_null_species_cd(row):\n",
    "    if pd.isna(row['SPECIES_CD']):\n",
    "        key = (row['TSA_NUMBER'], row['BEC_ZONE_C'])\n",
    "        # print(f\"Row: {row}, Key: {key}\")  # Debug statement\n",
    "        return species_mapping.get(key, row['SPECIES_CD'])\n",
    "    return row['SPECIES_CD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf3d76e-e954-43d5-bd6d-482c4e22b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands['SPECIES_CD'] = stands.apply(update_null_species_cd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd858993-e9ad-4722-9839-b4663f1d2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty GeoDataFrame\n",
      "Columns: [TSA_NUMBER, FEATURE_AR, BCLCS_LEVE, BEC_ZONE_C, SITE_INDEX, SPECIES_CD, SPECIES_PC, SPECIES__1, SPECIES__2, Shape_Leng, Shape_Area, contclass, rollup, netdown, THLB_Area, Block_ID, Age_2024, geometry]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# check if 'SPECIES_CD' is still None\n",
    "print(stands[stands['SPECIES_CD'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70dc1678-7d76-4790-ace5-09574cccc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands['BEC_ZONE_CODE_lexmatch'] = stands.BEC_ZONE_C.str.ljust(4, fillchar='x')\n",
    "\n",
    "stands['SPECIES_CD_1_lexmatch'] = stands['SPECIES_CD'].str.ljust(4, 'x')\n",
    "stands['SPECIES_CD_1_lexmatch'] = stands['SPECIES_CD'].str[:1] + stands['SPECIES_CD']\n",
    "\n",
    "stands['SPECIES_CD_2_lexmatch'] = stands['SPECIES__1'].str.ljust(4, 'x')\n",
    "stands['SPECIES_CD_2_lexmatch'] = stands['SPECIES__1'].str[:1] + stands['SPECIES__1']\n",
    "\n",
    "stratify_stand = stratify_stand\n",
    "stratify_stand_lexmatch = partial(stratify_stand, lexmatch=True)\n",
    "\n",
    "stands['stratum'] = stands.apply(stratify_stand, axis=1)\n",
    "stands['stratum_lexmatch'] = stands.apply(stratify_stand_lexmatch, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde6cfb-149a-412f-a150-265f2f18c47c",
   "metadata": {},
   "source": [
    "Read yield data from a CSV file and recast AU column data type to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3baffff5-2a29-459a-a6e0-a0fa7585ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process yld_vdyp for each TSA\n",
    "def process_yld_vdyp_dataframe(path, start_value):\n",
    "    df = pd.read_feather(path)\n",
    "    df['canfi_species'] = df.apply(lambda row: canfi_species(row['stratum_code']), axis=1).astype(int)\n",
    "    df['au_vdyp'] = pd.factorize(df['stratum_code'] + '_' + df['si_level'])[0] + start_value\n",
    "    return df\n",
    "\n",
    "paths = {\n",
    "    'yld_vdyp_03': (vdyp_curves_smooth_tsa03_path, 1),\n",
    "    'yld_vdyp_12': (vdyp_curves_smooth_tsa12_path, 46),\n",
    "    'yld_vdyp_14': (vdyp_curves_smooth_tsa14_path, 91),\n",
    "    'yld_vdyp_20': (vdyp_curves_smooth_tsa20_path, 181),\n",
    "    'yld_vdyp_24': (vdyp_curves_smooth_tsa24_path, 225),\n",
    "}\n",
    "\n",
    "dataframes = {name: process_yld_vdyp_dataframe(path, start_value) for name, (path, start_value) in paths.items()}\n",
    "\n",
    "yld_vdyp_03 = dataframes['yld_vdyp_03']\n",
    "yld_vdyp_12 = dataframes['yld_vdyp_12']\n",
    "yld_vdyp_14 = dataframes['yld_vdyp_14']\n",
    "yld_vdyp_20 = dataframes['yld_vdyp_20']\n",
    "yld_vdyp_24 = dataframes['yld_vdyp_24']\n",
    "\n",
    "# print(yld_vdyp_03.head())\n",
    "# print(yld_vdyp_12.head())\n",
    "# print(yld_vdyp_14.head())\n",
    "# print(yld_vdyp_20.head())\n",
    "# print(yld_vdyp_24.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78908ae2-294b-4549-af1c-2d67e8c35e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some modification on yld_vdyp_12 to remove the stratum codes that are not in the satnds' stratum\n",
    "\n",
    "yld_vdyp_12= yld_vdyp_12[~yld_vdyp_12['stratum_code'].isin(['SBS_SX', 'SBS_PLI', 'SBS_AT', 'SBS_BL', 'ESSF_SE', 'ESSF_PLI', 'SBS_AT+SX', 'SBS_SX+AT', 'SBS_PL'])]\n",
    "# yld_vdyp_12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f6d70-86c8-458f-b45c-48f2c658bc43",
   "metadata": {},
   "source": [
    "Now we merge all yiled curves for all TSAs into one data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50a833a-6b9c-46cd-afe6-3d43f49e3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yld_vdyp_14= yld_vdyp_14[~yld_vdyp_14['stratum_code'].isin(['ICH_HW', 'ICH_AT', 'ICH_SX', 'ICH_BL', 'ESSF_B', 'ICH_PL', 'ICH_H', 'ICH_AC', 'ICH_EP', 'SBS_ACT', 'SBS_SX+ACT', 'ICH_PLI', 'ICH_S'])]\n",
    "# yld_vdyp_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "254ebe6f-1ed8-4e30-8bec-36e677b21ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yld_vdyp_20= yld_vdyp_20[~yld_vdyp_20['stratum_code'].isin(['ICH_HW', 'ICH_AT', 'ICH_SX', 'ICH_BL'])]\n",
    "# yld_vdyp_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02a8b56e-8001-45c1-9737-48068bca6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "yld_vdyp_24= yld_vdyp_24[~yld_vdyp_24['stratum_code'].isin(['ICH_HW'])]\n",
    "# yld_vdyp_24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a7d73-2814-4282-8617-e09366c206e0",
   "metadata": {},
   "source": [
    "Here we need to split the satnds based on the TSA number to reduce the compution time for lexmatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28819aee-483a-42c2-8d6d-fe9d1be380c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_col = 'stratum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfe35b35-3820-4591-881b-bcb7aac425c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands_ = stands.reset_index().set_index(stratum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "953a8116-3d08-468d-bf0b-17ceef71ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsa_numbers = ['03', '12', '14', '20', '24']\n",
    "\n",
    "stands_dict = {}\n",
    "for tsa in tsa_numbers:\n",
    "    stands_dict[f'stands_{tsa}_'] = stands_[stands_['TSA_NUMBER'].isin([tsa])]\n",
    "\n",
    "stands_03_ = stands_dict['stands_03_']\n",
    "stands_12_ = stands_dict['stands_12_']\n",
    "stands_14_ = stands_dict['stands_14_']\n",
    "stands_20_ = stands_dict['stands_20_']\n",
    "stands_24_ = stands_dict['stands_24_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7df94d0b-5fc8-4ea9-8503-64b998b3b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "yld_vdyp_dict = {}\n",
    "\n",
    "for tsa in tsa_numbers:\n",
    "    df_name = f'yld_vdyp_{tsa}'\n",
    "    yld_vdyp_dict[f'yld_vdyp_{tsa}_'] = globals()[df_name].reset_index().set_index('stratum_code')\n",
    "\n",
    "yld_vdyp_03_ = yld_vdyp_dict['yld_vdyp_03_']\n",
    "yld_vdyp_12_ = yld_vdyp_dict['yld_vdyp_12_']\n",
    "yld_vdyp_14_ = yld_vdyp_dict['yld_vdyp_14_']\n",
    "yld_vdyp_20_ = yld_vdyp_dict['yld_vdyp_20_']\n",
    "yld_vdyp_24_ = yld_vdyp_dict['yld_vdyp_24_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dfc89ea-90e2-4215-829a-a39a2bd2f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start = time.time()\n",
    "# names1_03 = set(stands_03_.loc[yld_vdyp_03_.index.values].stratum_lexmatch.unique())\n",
    "# print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# names1_03\n",
    "\n",
    "\n",
    "# It took 37 minutes to run this script. so we keep the copy of output here for later. \n",
    "\n",
    "# {'ESSFESSFESSF_BBLBBL',\n",
    "#  'ESSFESSFESSF_PPLIPPLI',\n",
    "#  'ESSFESSFESSF_SSESSE',\n",
    "#  'ESSFESSFESSF_SSXSSX',\n",
    "#  'ICHxICHxICHx_AATAAT',\n",
    "#  'ICHxICHxICHx_BBLBBL',\n",
    "#  'ICHxICHxICHx_HHWHHW',\n",
    "#  'ICHxICHxICHx_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_AATAAT',\n",
    "#  'SBSxSBSxSBSx_AATAAT+SSX',\n",
    "#  'SBSxSBSxSBSx_BBLBBL',\n",
    "#  'SBSxSBSxSBSx_PPLIPPLI',\n",
    "#  'SBSxSBSxSBSx_PPLPPL',\n",
    "#  'SBSxSBSxSBSx_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_SSXSSX+AAT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41efc488-a7b0-499e-a0e8-431de1b0a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start = time.time()\n",
    "# names1_12 = set(stands_12_.loc[yld_vdyp_12_.index.values].stratum_lexmatch.unique())\n",
    "# print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# names1_12\n",
    "\n",
    "# It took 2.5 minutes to run this script.\n",
    "# {'ESSFESSFESSF_BBLBBL',\n",
    "#  'ESSFESSFESSF_SSXSSX',\n",
    "#  'ICHxICHxICHx_AATAAT',\n",
    "#  'ICHxICHxICHx_BBLBBL',\n",
    "#  'ICHxICHxICHx_HHWHHW',\n",
    "#  'ICHxICHxICHx_SSXSSX'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48401e0b-a9b6-4abd-a455-de674fc16463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start = time.time()\n",
    "# names1_14 = set(stands_14_.loc[yld_vdyp_14_.index.values].stratum_lexmatch.unique())\n",
    "# print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# names1_14\n",
    "\n",
    "# It took 127.2 minutes to run this script.\n",
    "# {'ESSFESSFESSF_BBLBBL',\n",
    "#  'ESSFESSFESSF_PPLIPPLI',\n",
    "#  'ESSFESSFESSF_SSESSE',\n",
    "#  'ESSFESSFESSF_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_AACAAC',\n",
    "#  'SBSxSBSxSBSx_AATAAT',\n",
    "#  'SBSxSBSxSBSx_AATAAT+PPLI',\n",
    "#  'SBSxSBSxSBSx_AATAAT+SSX',\n",
    "#  'SBSxSBSxSBSx_BBLBBL',\n",
    "#  'SBSxSBSxSBSx_FFDIFFDI',\n",
    "#  'SBSxSBSxSBSx_PPLIPPLI',\n",
    "#  'SBSxSBSxSBSx_PPLIPPLI+AAT',\n",
    "#  'SBSxSBSxSBSx_PPLPPL',\n",
    "#  'SBSxSBSxSBSx_SSBSSB',\n",
    "#  'SBSxSBSxSBSx_SSESSE',\n",
    "#  'SBSxSBSxSBSx_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_SSXSSX+AAT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7ecc249-06e1-47d9-8c30-2586bc431585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start = time.time()\n",
    "# names1_20 = set(stands_20_.loc[yld_vdyp_20_.index.values].stratum_lexmatch.unique())\n",
    "# print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# names1_20\n",
    "\n",
    "# It took 177.8 minutes to run this script.\n",
    "# {'ESSFESSFESSF_BBLBBL',\n",
    "#  'ESSFESSFESSF_PPLIPPLI',\n",
    "#  'ESSFESSFESSF_SSESSE',\n",
    "#  'ESSFESSFESSF_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_AATAAT',\n",
    "#  'SBSxSBSxSBSx_AATAAT+SSX',\n",
    "#  'SBSxSBSxSBSx_BBLBBL',\n",
    "#  'SBSxSBSxSBSx_PPLIPPLI',\n",
    "#  'SBSxSBSxSBSx_PPLPPL',\n",
    "#  'SBSxSBSxSBSx_SSXSSX',\n",
    "#  'SBSxSBSxSBSx_SSXSSX+AAT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b0eaa5c-4e9f-4334-9c4f-4e1a8e1f0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start = time.time()\n",
    "# names1_24 = set(stands_24_.loc[yld_vdyp_24_.index.values].stratum_lexmatch.unique())\n",
    "# print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")\n",
    "# names1_24\n",
    "\n",
    "# It took 0.5 minutes to run this script.\n",
    "# {'ESSFESSFESSF_BBLBBL',\n",
    "#  'ESSFESSFESSF_PPLIPPLI',\n",
    "#  'ESSFESSFESSF_SSESSE',\n",
    "#  'SBSxSBSxSBSx_AATAAT',\n",
    "#  'SBSxSBSxSBSx_BBLBBL',\n",
    "#  'SBSxSBSxSBSx_PPLIPPLI',\n",
    "#  'SBSxSBSxSBSx_SSXSSX'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b345a71f-c53d-44f3-ba2e-b089ebfaf445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ESSFESSFESSF_BBLBBL',\n",
       " 'ESSFESSFESSF_PPLIPPLI',\n",
       " 'ESSFESSFESSF_SSESSE',\n",
       " 'ESSFESSFESSF_SSXSSX',\n",
       " 'ICHxICHxICHx_AATAAT',\n",
       " 'ICHxICHxICHx_BBLBBL',\n",
       " 'ICHxICHxICHx_HHWHHW',\n",
       " 'ICHxICHxICHx_SSXSSX',\n",
       " 'SBSxSBSxSBSx_AACAAC',\n",
       " 'SBSxSBSxSBSx_AATAAT',\n",
       " 'SBSxSBSxSBSx_AATAAT+PPLI',\n",
       " 'SBSxSBSxSBSx_AATAAT+SSX',\n",
       " 'SBSxSBSxSBSx_BBLBBL',\n",
       " 'SBSxSBSxSBSx_FFDIFFDI',\n",
       " 'SBSxSBSxSBSx_PPLIPPLI',\n",
       " 'SBSxSBSxSBSx_PPLIPPLI+AAT',\n",
       " 'SBSxSBSxSBSx_PPLPPL',\n",
       " 'SBSxSBSxSBSx_SSBSSB',\n",
       " 'SBSxSBSxSBSx_SSESSE',\n",
       " 'SBSxSBSxSBSx_SSXSSX',\n",
       " 'SBSxSBSxSBSx_SSXSSX+AAT'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The values here have been obtained from the previous command cells to reduce runtime for the next runs.\n",
    "\n",
    "names1_03 = {'ESSFESSFESSF_BBLBBL',\n",
    " 'ESSFESSFESSF_PPLIPPLI',\n",
    " 'ESSFESSFESSF_SSESSE',\n",
    " 'ESSFESSFESSF_SSXSSX',\n",
    " 'ICHxICHxICHx_AATAAT',\n",
    " 'ICHxICHxICHx_BBLBBL',\n",
    " 'ICHxICHxICHx_HHWHHW',\n",
    " 'ICHxICHxICHx_SSXSSX',\n",
    " 'SBSxSBSxSBSx_AATAAT',\n",
    " 'SBSxSBSxSBSx_AATAAT+SSX',\n",
    " 'SBSxSBSxSBSx_BBLBBL',\n",
    " 'SBSxSBSxSBSx_PPLIPPLI',\n",
    " 'SBSxSBSxSBSx_PPLPPL',\n",
    " 'SBSxSBSxSBSx_SSXSSX',\n",
    " 'SBSxSBSxSBSx_SSXSSX+AAT'}\n",
    "\n",
    "names1_12 = {'ESSFESSFESSF_BBLBBL',\n",
    " 'ESSFESSFESSF_SSXSSX',\n",
    " 'ICHxICHxICHx_AATAAT',\n",
    " 'ICHxICHxICHx_BBLBBL',\n",
    " 'ICHxICHxICHx_HHWHHW',\n",
    " 'ICHxICHxICHx_SSXSSX'}\n",
    "\n",
    "names1_14 = {'ESSFESSFESSF_BBLBBL',\n",
    " 'ESSFESSFESSF_PPLIPPLI',\n",
    " 'ESSFESSFESSF_SSESSE',\n",
    " 'ESSFESSFESSF_SSXSSX',\n",
    " 'SBSxSBSxSBSx_AACAAC',\n",
    " 'SBSxSBSxSBSx_AATAAT',\n",
    " 'SBSxSBSxSBSx_AATAAT+PPLI',\n",
    " 'SBSxSBSxSBSx_AATAAT+SSX',\n",
    " 'SBSxSBSxSBSx_BBLBBL',\n",
    " 'SBSxSBSxSBSx_FFDIFFDI',\n",
    " 'SBSxSBSxSBSx_PPLIPPLI',\n",
    " 'SBSxSBSxSBSx_PPLIPPLI+AAT',\n",
    " 'SBSxSBSxSBSx_PPLPPL',\n",
    " 'SBSxSBSxSBSx_SSBSSB',\n",
    " 'SBSxSBSxSBSx_SSESSE',\n",
    " 'SBSxSBSxSBSx_SSXSSX',\n",
    " 'SBSxSBSxSBSx_SSXSSX+AAT'}\n",
    "\n",
    "names1_20 = {'ESSFESSFESSF_BBLBBL',\n",
    " 'ESSFESSFESSF_PPLIPPLI',\n",
    " 'ESSFESSFESSF_SSESSE',\n",
    " 'ESSFESSFESSF_SSXSSX',\n",
    " 'SBSxSBSxSBSx_AATAAT',\n",
    " 'SBSxSBSxSBSx_AATAAT+SSX',\n",
    " 'SBSxSBSxSBSx_BBLBBL',\n",
    " 'SBSxSBSxSBSx_PPLIPPLI',\n",
    " 'SBSxSBSxSBSx_PPLPPL',\n",
    " 'SBSxSBSxSBSx_SSXSSX',\n",
    " 'SBSxSBSxSBSx_SSXSSX+AAT'}\n",
    "\n",
    "names1_24 = {'ESSFESSFESSF_BBLBBL',\n",
    " 'ESSFESSFESSF_PPLIPPLI',\n",
    " 'ESSFESSFESSF_SSESSE',\n",
    " 'SBSxSBSxSBSx_AATAAT',\n",
    " 'SBSxSBSxSBSx_BBLBBL',\n",
    " 'SBSxSBSxSBSx_PPLIPPLI',\n",
    " 'SBSxSBSxSBSx_SSXSSX'}\n",
    "\n",
    "names1 = names1_03.union(names1_12, names1_14, names1_20, names1_24)\n",
    "\n",
    "names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aef63f1b-51fd-4b91-8473-f5ddf9fd84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalarea = stands_.FEATURE_AR.sum()\n",
    "stands_['totalarea_p'] = stands_.FEATURE_AR / totalarea\n",
    "names2 = set(stands_.stratum_lexmatch.unique()) - names1\n",
    "stratum_key = stands_.reset_index().groupby('%s_lexmatch' % stratum_col)[stratum_col].first()\n",
    "totalarea_p_sum__ = stands_.groupby('%s_lexmatch' % stratum_col).totalarea_p.sum()\n",
    "lev_dist = {n2:{n1:distance.levenshtein(n1, n2) for n1 in names1} for n2 in names2} \n",
    "lev_dist_low = {n2:{n1:(lev_dist[n2][n1], totalarea_p_sum__.loc[n1]) \n",
    "                    for n1 in lev_dist[n2].keys() if lev_dist[n2][n1] == min(lev_dist[n2].values())} \n",
    "                for n2 in names2}\n",
    "best_match = {stratum_key.loc[n2]:stratum_key[max(lev_dist_low[n2].items(), key=operator.itemgetter(1))[0]] for n2 in names2}\n",
    "stands_.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2225e497-92fe-461d-b455-fa02b7369e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>age</th>\n",
       "      <th>volume</th>\n",
       "      <th>si_level</th>\n",
       "      <th>canfi_species</th>\n",
       "      <th>au_vdyp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stratum_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SBS_SX</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1.062753e-13</td>\n",
       "      <td>L</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBS_SX</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4.118229e-10</td>\n",
       "      <td>L</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBS_SX</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1.858092e-08</td>\n",
       "      <td>L</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBS_SX</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2.237515e-07</td>\n",
       "      <td>L</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SBS_SX</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1.413441e-06</td>\n",
       "      <td>L</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESSF_PLI</th>\n",
       "      <td>7029</td>\n",
       "      <td>294</td>\n",
       "      <td>295</td>\n",
       "      <td>2.974311e+02</td>\n",
       "      <td>H</td>\n",
       "      <td>204</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESSF_PLI</th>\n",
       "      <td>7030</td>\n",
       "      <td>295</td>\n",
       "      <td>296</td>\n",
       "      <td>2.965865e+02</td>\n",
       "      <td>H</td>\n",
       "      <td>204</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESSF_PLI</th>\n",
       "      <td>7031</td>\n",
       "      <td>296</td>\n",
       "      <td>297</td>\n",
       "      <td>2.957375e+02</td>\n",
       "      <td>H</td>\n",
       "      <td>204</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESSF_PLI</th>\n",
       "      <td>7032</td>\n",
       "      <td>297</td>\n",
       "      <td>298</td>\n",
       "      <td>2.948842e+02</td>\n",
       "      <td>H</td>\n",
       "      <td>204</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESSF_PLI</th>\n",
       "      <td>7033</td>\n",
       "      <td>298</td>\n",
       "      <td>299</td>\n",
       "      <td>2.940268e+02</td>\n",
       "      <td>H</td>\n",
       "      <td>204</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49197 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              level_0  index  age        volume si_level  canfi_species  \\\n",
       "stratum_code                                                              \n",
       "SBS_SX              0      8    9  1.062753e-13        L            100   \n",
       "SBS_SX              1      9   10  4.118229e-10        L            100   \n",
       "SBS_SX              2     10   11  1.858092e-08        L            100   \n",
       "SBS_SX              3     11   12  2.237515e-07        L            100   \n",
       "SBS_SX              4     12   13  1.413441e-06        L            100   \n",
       "...               ...    ...  ...           ...      ...            ...   \n",
       "ESSF_PLI         7029    294  295  2.974311e+02        H            204   \n",
       "ESSF_PLI         7030    295  296  2.965865e+02        H            204   \n",
       "ESSF_PLI         7031    296  297  2.957375e+02        H            204   \n",
       "ESSF_PLI         7032    297  298  2.948842e+02        H            204   \n",
       "ESSF_PLI         7033    298  299  2.940268e+02        H            204   \n",
       "\n",
       "              au_vdyp  \n",
       "stratum_code           \n",
       "SBS_SX              1  \n",
       "SBS_SX              1  \n",
       "SBS_SX              1  \n",
       "SBS_SX              1  \n",
       "SBS_SX              1  \n",
       "...               ...  \n",
       "ESSF_PLI          248  \n",
       "ESSF_PLI          248  \n",
       "ESSF_PLI          248  \n",
       "ESSF_PLI          248  \n",
       "ESSF_PLI          248  \n",
       "\n",
       "[49197 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the DataFrames vertically\n",
    "yld_vdyp_ = pd.concat([yld_vdyp_03_, yld_vdyp_12_, yld_vdyp_14_, yld_vdyp_20_, yld_vdyp_24_], axis=0)\n",
    "yld_vdyp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88c7f3a1-5f7c-4593-94ca-084122480bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 4.7 minutes to run this script.\n"
     ]
    }
   ],
   "source": [
    "Start = time.time()\n",
    "def match_stratum(r):\n",
    "    return r[stratum_col] if r[stratum_col] in yld_vdyp_.index.values else best_match[r[stratum_col]]\n",
    "\n",
    "stands_['%s_matched' % stratum_col] = stands_.apply(match_stratum, axis=1)\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc546073-00ed-4288-9c55-c990cef011ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands_ = stands_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5bf3383-57bb-45f6-a40e-1e4a3c850ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_col = '%s_matched' % stratum_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42be7eb3-2d75-4751-97e9-03e3a9f0b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands__ = stands_.set_index(stratum_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae292b31-2d63-4555-a105-53c9215b84c3",
   "metadata": {},
   "source": [
    "Compile site index (SI) stats, including quantile levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7256bf5a-1742-476b-a57d-9e1fa5de5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratum_si_stats = stands__.groupby(stratum_col).SITE_INDEX.describe(percentiles=[0, 0.05, 0.20, 0.35, 0.5, 0.65, 0.80, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eeed00e5-bff3-4305-b64a-237a354f546b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 2.3 minutes to run this script.\n"
     ]
    }
   ],
   "source": [
    "Start = time.time()\n",
    "\n",
    "# Function to find the closest column in stratum_si_stats and determine the level\n",
    "def find_closest_column(row):\n",
    "    si_levelquants = {'L': [0, 5, 20, 35], 'M': [35, 50, 65], 'H': [65, 80, 95, 100]}\n",
    "    stratum_matched = row['stratum_matched']\n",
    "    site_index = row['SITE_INDEX']   \n",
    "    abs_diff = abs(site_index - stratum_si_stats.loc[stratum_matched, ['0%', '5%', '20%', '35%', '50%', '65%', '80%', '95%', '100%']])    \n",
    "    closest_column = abs_diff.idxmin()    \n",
    "    for level, quants in si_levelquants.items():\n",
    "        if int(closest_column[:-1]) in quants:\n",
    "            return level\n",
    "\n",
    "stands_['si_level'] = stands_.apply(find_closest_column, axis=1)\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5528b99e-f65c-42b5-a537-dcf84df797d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yld_vdyp = pd.concat([yld_vdyp_03, yld_vdyp_12, yld_vdyp_14, yld_vdyp_20, yld_vdyp_24], axis=0)\n",
    "# yld_vdyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "968c24d3-1e89-4f3f-9440-ed52067e64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Start = time.time()\n",
    "\n",
    "au_vdyp_values = []\n",
    "for index, row in stands_.iterrows():\n",
    "    filtered_df = yld_vdyp[(yld_vdyp['stratum_code'] == row['stratum_matched']) & (yld_vdyp['si_level'] == row['si_level'])]    \n",
    "    if not filtered_df.empty:\n",
    "        # Get the value of au_vdyp from the first DataFrame and append it to the list\n",
    "        au_vdyp_values.append(filtered_df['au_vdyp'].values[0])\n",
    "    else:\n",
    "        # If no matching rows found, append NaN\n",
    "        au_vdyp_values.append(None)\n",
    "\n",
    "# Add the new column 'au_vdyp' to the second DataFrame with the extracted values\n",
    "stands_['au_vdyp'] = au_vdyp_values\n",
    "\n",
    "print('It took', round((time.time() - Start) / 60, 1), \"minutes to run this script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a2760f3-9ea8-4e27-a304-00c1b6476721",
   "metadata": {},
   "outputs": [],
   "source": [
    "stands_[['BEC_zone_matched', 'species_matched']] = stands_['stratum_matched'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18743f48-8ca4-4725-8f4d-5f5e654b76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['TSA_NUMBER', 'contclass', 'Age_2024', 'geometry', 'species_matched', 'au_vdyp']\n",
    "stands_mdf = stands_[columns_to_keep].copy()\n",
    "stands_mdf.loc[:,'area'] = stands_mdf.geometry.area * 0.0001 # monkey-patch broken area attribute\n",
    "stands_mdf =  stands_mdf.rename(columns={'TSA_NUMBER': 'theme0', 'contclass':'theme1', 'Age_2024':'age', 'species_matched':'species', 'au_vdyp':'theme5'})\n",
    "stands_mdf['theme0'] = stands_mdf['theme0'].replace({'03': 'tsa03', '12': 'tsa12', '14': 'tsa14', '20': 'tsa20', '24': 'tsa24'})\n",
    "stands_mdf['theme1'] = stands_mdf['theme1'].replace({'C': 1, 'N': 0})\n",
    "stands_mdf['theme2'] = stands_mdf['theme5']\n",
    "stands_mdf = stands_mdf.drop(columns='geometry')\n",
    "stands_mdf.insert(4, 'theme3',  stands_mdf['species'].map(canfi_map)) #Burn CANFI species codes into stand data\n",
    "stands_mdf['theme3'] = stands_mdf['theme3'].astype(int)\n",
    "stands_mdf.drop(columns=['species'], inplace=True)\n",
    "stands_mdf.insert(5, 'theme4', stands_mdf['theme5']) # to be filled out with the scpecies code\n",
    "stands_mdf.insert(6, 'age', stands_mdf.pop('age'))\n",
    "stands_mdf = stands_mdf.loc[:, ['theme0', 'theme1', 'theme2', 'theme3', 'theme4', 'theme5', 'area', 'age']]\n",
    "stands_mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea058d-f911-47da-925b-eae5c7b48c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('whole area (ha) is:', stands_mdf['area'].sum())\n",
    "print('whole contributing area (ha) is:', stands_mdf[stands_mdf['theme1'] == 1]['area'].sum())\n",
    "print('whole non-contributing area (ha) is:', stands_mdf[stands_mdf['theme1'] == 0]['area'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744ac82-6f68-4c3a-8ae6-9ab7816af43c",
   "metadata": {},
   "source": [
    "Create analysis unit (AU) dataframe from stands dataframe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa76a9f6-1e84-49c7-b8e2-ad45ae938613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AU = pd.DataFrame(stands_mdf['theme5']).drop_duplicates()\n",
    "AU.rename(columns={'theme5':'au_vdyp'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f1deb-4de4-41ef-ace3-a49e5c10b5e1",
   "metadata": {},
   "source": [
    "Join `AU` and `yld_vdyp` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7280c970-ea89-42fd-9d81-cc9507c9edd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yldmerged = pd.merge(AU, yld_vdyp, on=['au_vdyp'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e96c8b0c-73fc-4be7-a27c-09da38b6ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "au_yldmerged = yldmerged['au_vdyp'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27339769-c706-40d4-9713-8719af8e35c5",
   "metadata": {},
   "source": [
    "Add a new `curve_id` colume that has same data values as `AU` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71de783f-3fe3-4815-a6fa-b4249aa3da67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yldmerged['curve_id'] = yldmerged['au_vdyp'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c94ba-1e79-44f5-ac6c-e3869dfe7972",
   "metadata": {},
   "source": [
    "Rename stuff to match variable names we expect further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2bfe0e7d-f196-4b29-9f5c-98ab933e0a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stands_table = stands_mdf\n",
    "curve_points_table = yldmerged\n",
    "curve_points_table.set_index('au_vdyp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1cedd2-6aa2-492d-b3a2-2f9d3f5e591e",
   "metadata": {},
   "source": [
    "# Export Woodstock-formatted input files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683c6c4-9313-437b-ba2f-7e9fc498b91b",
   "metadata": {},
   "source": [
    "We can use the new ws3 model instance we just built to export ws3 input files in Woodstock file format. We do this for three reasons. \n",
    "\n",
    "The first reason is that it will be simpler and more compact in the actual DSS notebook to instantiate the `ForestModel` object from these Woodstock-formatted files (and also this will provide an opportunity to demonstrate the existance and usage of the Woodstock model import functions that are built into ws3). \n",
    "\n",
    "The second reason is that the process of exporting data from a live `ws3.forest.ForestModel` instance to Woodstock-formatted input data files provides some insight into the internal structure and workings of ws3 models (which can be a challenging thing to get started with, particularly if you do not have a lot of experience building and running forest estate models). \n",
    "\n",
    "The third reason is that Woodstock file format is designed to be \"human readable\" (sort of... nobody ever said it would be super easy or super fun). Picking through the exported Woodstock-formatted files might help some people better understand the structure and details of the model we have built. If you have no experience reading Woodstock-formatted model input data files, then this is going to be trickier (unless you pause here and go take an introductory Woodstock training course of sort). Many forest professionals already have familiarity with Woodstock software and its special file format (through having been exposed to this at some point in their career). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c2463-6bd1-4f48-9be6-e9a4621281c7",
   "metadata": {},
   "source": [
    "Start by creating a new subdirectory to hold the new Woodstock-formatted data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c020155-9642-42ed-9519-f0aaa9985648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir data/woodstock_model_files_mining_site_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc5229-5643-44e7-a872-4031a0f535e0",
   "metadata": {},
   "source": [
    "## LANDSCAPE section\n",
    "\n",
    "The LANDSCAPE section defines stratification variables (themes) and stratification variable values (basecodes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30717fff-79f5-4b46-8a3a-ffadea779386",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_cols=['theme0', # TSA \n",
    "            'theme1', # THLB\n",
    "            'theme2', # TIPSY AUs\n",
    "            'theme3', # leading species code\n",
    "            'theme4',  # yield curve ID\n",
    "            'theme5' # VDYP Aus\n",
    "           ]\n",
    "basecodes = [list(map(lambda x: str(x), stands_table[tc].unique())) for tc in theme_cols]\n",
    "basecodes[2] = list(set(basecodes[2] + list(stands_table['theme2'].astype(str))))\n",
    "basecodes[3] = list(set(basecodes[3] + list(stands_table['theme3'].astype(str))))\n",
    "basecodes[4] = list(set(basecodes[4] + list(stands_table['theme4'].astype(str))))\n",
    "basecodes[5] = list(set(basecodes[5] + list(stands_table['theme5'].astype(str))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab4e1957-6ebd-47ab-aa3e-bff2c2ab78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(woodstock_model_files_lan_path, 'w') as file:\n",
    "    print('*THEME Timber Supply Area (TSA)', file=file)\n",
    "    tsas = ['tsa03', 'tsa12', 'tsa14', 'tsa20', 'tsa24']\n",
    "    for tsa in tsas:\n",
    "        print(tsa, file=file)\n",
    "    print('*THEME Timber Harvesting Land Base (THLB)', file=file)\n",
    "    for basecode in basecodes[1]: print(basecode, file=file)\n",
    "    print('*THEME Analysis Unit (AU_TIPSY)', file=file)\n",
    "    for basecode in basecodes[2]: print(basecode, file=file)\n",
    "    print('*THEME Leading tree species (CANFI species code)', file=file)\n",
    "    for basecode in basecodes[3]: print(basecode, file=file)\n",
    "    print('*THEME Yield curve ID', file=file)\n",
    "    for basecode in basecodes[4]: print(basecode, file=file)\n",
    "    print('*THEME Analysis Unit (AU_VDYP)', file=file)\n",
    "    for basecode in basecodes[5]: print(basecode, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7bdaf9-e12f-4d7f-9614-fea6953b7a70",
   "metadata": {},
   "source": [
    "## AREAS section\n",
    "\n",
    "The AREAS section defines the initial forest inventory, in terms of how many hectares of which age class are present in which development type (where a development type is defined as a unique sequence of landscape theme variable values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ac4c7ca-3404-4eb8-accb-a440dc2490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gstands = stands_table.groupby(theme_cols+['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2c3f44f-0a84-43b7-a13d-db7dda4342da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(woodstock_model_files_are_path, 'w') as file:\n",
    "    for name, group in gstands:\n",
    "        dtk, age, area = tuple(map(lambda x: str(x), name[:-1])), int(name[-1]), group['area'].sum()\n",
    "        print('*A', ' '.join(v for v in dtk), age, area, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc58db-0ed1-45da-8165-4152a4ad2f60",
   "metadata": {},
   "source": [
    "## YIELDS section\n",
    "\n",
    "The YIELDS section defines yield curves (in this example we only track merchantable log volume, but we can use yield curves to track all sorts of other stuff). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7e26ee4-9770-4715-b93d-1ee46c3bd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(woodstock_model_files_yld_path, 'w') as file:\n",
    "    tot=[]\n",
    "    swd=[]\n",
    "    hwd=[]\n",
    "    unique_au_rows = curve_points_table[~curve_points_table.index.duplicated(keep='first')]    \n",
    "    for AU, au_row in unique_au_rows.iterrows():\n",
    "        yname = 's%04d' % int(au_row.canfi_species)    \n",
    "        curve_id = au_row.curve_id\n",
    "        mask = ('?', '?', '?', '?', str(curve_id), str(AU) )\n",
    "        points = [(r.age, r.volume) for _, r in curve_points_table.loc[AU].iterrows() if not r.age % period_length and r.age <= max_age]\n",
    "        c = ws3.core.Curve(yname, points=points, type='a', is_volume=True, xmax=max_age, period_length=period_length)\n",
    "        print('*Y', ' '.join(v for v in mask), file=file)\n",
    "        print(yname, '1', ' '.join(str(int(c[x])) for x in range(0, 300, 10)), file=file)\n",
    "        if yname not in tot:\n",
    "            tot.append(yname)\n",
    "        if int(au_row.canfi_species) > 1200:\n",
    "            if yname not in hwd: hwd.append(yname)\n",
    "        else:\n",
    "            if yname not in swd: swd.append(yname)\n",
    "    print('*YC ? ? ? ? ? ?', file=file)\n",
    "    print('totvol _SUM(%s)' % ', '.join(map(str, tot)), file=file)\n",
    "    print('swdvol _SUM(%s)' % ', '.join(map(str, swd)), file=file)\n",
    "    print('hwdvol _SUM(%s)' % ', '.join(map(str, hwd)), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfebee-b98a-4f84-a4c8-0613f1c7cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(8, 12), sharex=True)\n",
    "\n",
    "cvol = c\n",
    "ccai = c.cai()\n",
    "cmai = c.mai()\n",
    "cmaiytp = c.mai().ytp()\n",
    "x_cmai = cmaiytp.lookup(0) # optimal rotation age (i.e., culmination of MAI curve)\n",
    "labels = 'total volume', 'MAI (and CAI)', 'MAI YTP'\n",
    "\n",
    "ax[0].plot(*zip(*c.points()))\n",
    "ax[0].plot([0, x_cmai], [0., cvol[x_cmai]], linestyle='--', color='green')\n",
    "\n",
    "ax[1].plot(*zip(*c.mai().points()))\n",
    "ax[1].plot(*zip(*c.cai().points()), linestyle=':')\n",
    "\n",
    "ax[2].plot(*zip(*c.mai().ytp().points()))\n",
    "ax[2].axhline(0, color='black')\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_ylabel(labels[i])\n",
    "    ax[i].set_ylim(0, None)\n",
    "    ax[i].axvline(x_cmai, color='red')\n",
    "plt.xlim(0, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd641b3-d2cb-4239-898c-5d65521276f7",
   "metadata": {},
   "source": [
    "## ACTIONS section\n",
    "\n",
    "The ACTIONS section defines actions that can be applied in the model (e.g., harvesting, planting, thinning, fertilization, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "32c46b4d-471f-4038-92dd-48a0751d8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(woodstock_model_files_act_path, 'w') as file:\n",
    "    print('ACTIONS', file=file)\n",
    "    print('*ACTION harvest Y', file=file)\n",
    "    print('*OPERABLE harvest', file=file)\n",
    "    print('? 1 ? ? ? ? _AGE >= 100 AND _AGE <= 600', file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0ae8e-5882-4248-b140-52db2b7ffb76",
   "metadata": {},
   "source": [
    "## TRANSITIONS section\n",
    "\n",
    "The TRANSITIONS section defines transitions (i.e., transition to a new development type and age class induced by applying a specific action to a specific combination of development type and age class). If there were no transitions in a forest estate model, it would simply be aging (i.e., growing) the forest forward from time step 1 through to time step N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e3d3258-750c-4de4-a07e-bdf6d8206138",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(woodstock_model_files_trn_path, 'w') as file:\n",
    "    acode = 'harvest'\n",
    "    print('*CASE', acode, file=file)\n",
    "    record_au = set()\n",
    "    for au_id, au_row in stands_table.iterrows():\n",
    "        if au_row.theme5 in record_au: continue\n",
    "        if not au_row.theme1: continue\n",
    "        target_curve_id = au_row.theme4  \n",
    "        smask = ' '.join(('?', '?', '?', '?', '?', str(target_curve_id)))\n",
    "        tmask = ' '.join(('?', '?' , '?', '?', str(target_curve_id), '?'))\n",
    "        print('*SOURCE', smask, file=file)\n",
    "        print('*TARGET', tmask, '100', file=file)\n",
    "        record_au.add(au_row.theme5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
